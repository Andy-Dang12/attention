### List of attention

- [x] pytorch
- [x] [Multi-head Attention](https://arxiv.org/abs/1706.03762)
- [x] [Multi-query Attention](https://arxiv.org/abs/1911.02150)
- [x] [Grouped-query Attention](https://arxiv.org/abs/2305.13245)
- [x] [Linear Attention & Multi-head Linear Attention](https://arxiv.org/abs/2006.04768)
- [ ] sliding window attention (SWA) = [sparse attention](https://arxiv.org/abs/1904.10509v1) & [Longformer](https://arxiv.org/abs/2004.05150)
- [ ] [Attention Sinks](https://arxiv.org/abs/2309.17453v1)
- [ ] [transpose self-attention](https://arxiv.org/abs/2206.10589v3)
- [ ] [Hierarchical Attention](https://arxiv.org/abs/2306.06189)
- [ ] [Agent Attention](https://arxiv.org/abs/2312.08874)
