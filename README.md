# multi-query-attention
Fast Transformer Decoding: One Write-Head is All You Need
