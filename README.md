# multi-query-attention
Fast Transformer Decoding: One Write-Head is All You Need

## Useful url

<details><summary> <b>Expand</b> </summary>

* Search for articles that cite this [article](https://arxiv.org/abs/1911.02150) , such as [PALM](https://arxiv.org/abs/2204.02311)
* a implement in [stackoverflow](https://stackoverflow.com/questions/76378844/multi-query-attention-implementation)
* a implement in [huggingface](https://huggingface.co/mosaicml/mpt-7b-chat/blob/main/attention.py#L204)
* a implement from [lucidrains/PaLM-pytorch](https://github.com/lucidrains/PaLM-pytorch/blob/main/palm_pytorch/palm_pytorch.py#L133)
</details>
